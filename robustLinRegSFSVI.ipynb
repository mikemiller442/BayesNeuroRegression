{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2950609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from torch.nn.utils.parametrizations import _Orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539cf84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined model class that inherits from Pytorch's neural network module\n",
    "# NumComponents is the number of low rank components to use in the low rank\n",
    "# part of the diagonal plus low rank covariance matrix.\n",
    "# numCESamples is the number of stochastic samples of the cross entropy\n",
    "# numNLLSamples is the number of stochastic samples of the negative log likelihood.\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,trainX,testX,trainY,testY,numComponents,numCESamples,numNLLSamples,xiVal):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input data\n",
    "        self.X = trainX\n",
    "        self.testX = testX\n",
    "        self.Y = trainY\n",
    "        self.testY = testY\n",
    "        self.trainN = self.Y.size(dim=0)\n",
    "        self.testN = self.testY.size(dim=0)\n",
    "        self.nw = self.X.size(dim=1)\n",
    "        self.nc = numComponents\n",
    "        self.ns = numCESamples\n",
    "        self.nsNLL = numNLLSamples\n",
    "        \n",
    "        # Regression parameter means\n",
    "        betaInit = torch.squeeze(torch.distributions.Uniform(-0.0001,0.0001).sample((1,self.nw)).double())\n",
    "        self.betaMu = nn.Parameter(betaInit)\n",
    "        self.scaleSqMu = nn.Parameter(torch.tensor([math.log(0.000025)-0.5,-0.5],dtype=torch.double))\n",
    "        self.xi = nn.Parameter(torch.tensor(xiVal,dtype=torch.double))\n",
    "        \n",
    "        # Diagonal component of parameter covariance\n",
    "        self.paramVar = nn.Parameter(torch.ones((self.nw+2),dtype=torch.double)*0.0001)\n",
    "        \n",
    "        # Low rank covariance of all parameters\n",
    "        self.V = nn.Parameter(torch.distributions.Uniform(-0.0001,0.0001).sample((self.nw+2,self.nc)).double())\n",
    "        \n",
    "        # Diagonal matrix of eigenvalues for low rank component\n",
    "        self.C = nn.Parameter(torch.ones(self.nc,dtype=torch.double)*-10.0)\n",
    "        \n",
    "        # Apply orthogonal parametrization to V\n",
    "        # Use householder reflections instead of the matrix exponential or Cayley map as householder\n",
    "        # reflections are computationally cheaper\n",
    "        parametrize.register_parametrization(self,\"V\", _Orthogonal(self.V, orthogonal_map=\"householder\"))\n",
    "        \n",
    "        # Scale parameter priors\n",
    "        self.lambdaPrior = torch.tensor(1.0,dtype=torch.double)\n",
    "        self.regVarPrior = torch.tensor(1.0,dtype=torch.double)\n",
    "        \n",
    "        # Training metrics\n",
    "        self.listTrainMSE = []\n",
    "        self.listTestMSE = []\n",
    "        self.listTrainR2 = []\n",
    "        self.listTestR2 = []\n",
    "        \n",
    "\n",
    "# Samples the weights wrt to the q density\n",
    "def sampleWeights(mod):\n",
    "    diagC = torch.diag(torch.exp(mod.C))\n",
    "    mod.paramCov = torch.diag(torch.exp(mod.paramVar)) + torch.matmul(torch.matmul(mod.V,diagC),torch.transpose(mod.V,0,1))\n",
    "    lsCov = mod.paramCov[0:2,0:2]\n",
    "     \n",
    "    predWeights = 0.0\n",
    "    for i in range(mod.ns):\n",
    "        \n",
    "        # Sample diagonal lambdaSq and sigmaSq\n",
    "        mod.stdNormalDistNP = torch.distributions.MultivariateNormal(mod.scaleSqMu, covariance_matrix = lsCov)\n",
    "        lsParams = mod.stdNormalDistNP.rsample()\n",
    "        # Sample weights\n",
    "        predBeta = (torch.unsqueeze(mod.betaMu,1) + torch.matmul(mod.paramCov[2:,0:2],\n",
    "                    torch.matmul(torch.inverse(lsCov),torch.unsqueeze(lsParams,1) - torch.unsqueeze(mod.scaleSqMu,1))))\n",
    "        scaleParams = torch.unsqueeze(torch.cat(((torch.unsqueeze(torch.tensor(1.0,dtype=torch.double),0)).clone(),\n",
    "                      torch.exp(0.5*mod.xi*lsParams[0])*torch.ones((mod.nw-1))),0),1)\n",
    "        predWeights = predWeights + torch.mul(predBeta,scaleParams)\n",
    "        \n",
    "    return (1.0/mod.ns)*predWeights\n",
    "\n",
    "\n",
    "# Calculates the negative log likelihood\n",
    "def NLL(mod,startBatch,endBatch):\n",
    "    diagC = torch.diag(torch.exp(mod.C))\n",
    "    diagC_sqrt = torch.diag(torch.sqrt(torch.exp(mod.C)))\n",
    "    mod.paramCov = torch.diag(torch.exp(mod.paramVar)) + torch.matmul(torch.matmul(mod.V,diagC),torch.transpose(mod.V,0,1))\n",
    "    lsCov = mod.paramCov[0:2,0:2]\n",
    "    \n",
    "    K = (torch.diag(torch.pow(torch.exp(mod.C), -1.0))\n",
    "         + torch.matmul(torch.matmul(torch.transpose(mod.V[0:2,:],0,1),\n",
    "                                     torch.diag(torch.pow(torch.exp(mod.paramVar[0:2]),-1.0))),mod.V[0:2,:]))\n",
    "    \n",
    "    LK = torch.linalg.cholesky(K)\n",
    "    mod.stdNormalDistNP = torch.distributions.MultivariateNormal(mod.scaleSqMu,covariance_matrix = lsCov)\n",
    "    \n",
    "    nll = 0.0\n",
    "    for i in range(mod.nsNLL):\n",
    "        \n",
    "        # Sample diagonal lambdaSq and sigmaSq\n",
    "        lsParams = mod.stdNormalDistNP.rsample(sample_shape=torch.Size([endBatch-startBatch]))\n",
    "        \n",
    "        ### Calculate pre-activation distribution\n",
    "        # calculate mean beta conditional on sampled scale parameters\n",
    "        predBeta = (torch.unsqueeze(mod.betaMu,1) + torch.matmul(mod.paramCov[2:,0:2],\n",
    "                    torch.matmul(torch.inverse(lsCov),torch.transpose(lsParams,0,1)\n",
    "                 - torch.unsqueeze(mod.scaleSqMu,1).repeat(1,endBatch-startBatch))))\n",
    "        scaleData = torch.cat(((torch.unsqueeze(mod.X[startBatch:endBatch,0],1)).clone(),\n",
    "                    torch.unsqueeze(torch.exp(0.5*mod.xi*lsParams[:,0]),1)*mod.X[startBatch:endBatch,1:]),1)\n",
    "        predMu = torch.sum(torch.mul(torch.transpose(predBeta,0,1),scaleData),dim=1)\n",
    "        \n",
    "        Y = torch.linalg.solve(torch.transpose(LK,0,1),\n",
    "                               torch.matmul(torch.matmul(torch.matmul(torch.transpose(mod.V[0:2,:],0,1),\n",
    "                                                                      torch.diag(torch.pow(torch.exp(mod.paramVar[0:2]),-1))),\n",
    "                                                         torch.matmul(mod.V[0:2,:], diagC_sqrt)),\n",
    "                                            torch.matmul(torch.matmul(diagC_sqrt, torch.transpose(mod.V[2:,:],0,1)),\n",
    "                                                         torch.transpose(scaleData,0,1))))\n",
    "        \n",
    "        # Standard deviation calculation with updated low-rank component\n",
    "        # Use sqrt(C) instead of C when calculating standard deviation of pre-activations\n",
    "        term1 = torch.sum(torch.square(torch.matmul(torch.diag(torch.exp(0.5*mod.paramVar[2:])),\n",
    "                                                    torch.transpose(scaleData,0,1))),dim=0)\n",
    "        term2 = torch.sum(torch.square(torch.matmul(torch.matmul(diagC_sqrt,torch.transpose(mod.V[2:,:],0,1)),\n",
    "                                                    torch.transpose(scaleData,0,1))),dim=0)\n",
    "        temp3 = torch.matmul(torch.matmul(diagC_sqrt,torch.transpose(mod.V[2:,:],0,1)),\n",
    "                             torch.transpose(scaleData,0,1))\n",
    "        temp3 = torch.matmul(torch.matmul(mod.V[0:2,:],diagC_sqrt),temp3)\n",
    "        temp3 = torch.matmul(torch.diag(torch.pow(torch.exp(mod.paramVar[0:2]),-0.5)),temp3)\n",
    "        term3 = torch.sum(torch.square(temp3),dim=0)\n",
    "        term4 = torch.sum(torch.square(Y), dim=0)\n",
    "\n",
    "        sdB = torch.sqrt(term1 + term2 - term3 + term4)\n",
    "        \n",
    "        # Sample pre-activation and calculate negative log-likelihood\n",
    "        mod.stdNormalDistB = torch.distributions.Normal(predMu,sdB)\n",
    "        sampleB = mod.stdNormalDistB.rsample()\n",
    "        trainResiduals = mod.Y[startBatch:endBatch,:] - torch.unsqueeze(sampleB,1)\n",
    "        SE = torch.square(trainResiduals)\n",
    "        # Calculate log likelihood\n",
    "        nll = (nll + torch.sum(torch.mul(0.5*torch.unsqueeze(torch.exp(-1.0*lsParams[:,1]),1),SE)\n",
    "            + 0.5*(math.log(2.0*math.pi) + torch.unsqueeze(lsParams[:,1],1))))\n",
    "        \n",
    "    return (1.0/mod.nsNLL)*nll\n",
    "\n",
    "\n",
    "# Calculates the cross entropy\n",
    "def crossEntropy(mod):\n",
    "        \n",
    "    ## Sample all parameters together\n",
    "    mod.stdNormalDistNS1 = torch.distributions.Normal(torch.zeros((mod.ns,mod.nw+2),dtype=torch.double),\n",
    "                                                      torch.ones((mod.ns,mod.nw+2),dtype=torch.double))\n",
    "    mod.stdNormalDistNS2 = torch.distributions.Normal(torch.zeros((mod.nc),dtype=torch.double),\n",
    "                                                      torch.ones((mod.nc),dtype=torch.double))\n",
    "    \n",
    "    # Sample all parameters with C matrix - VECTORIZED\n",
    "    diagC_sqrt = torch.sqrt(torch.exp(mod.C))\n",
    "\n",
    "    # Sample diagonal component\n",
    "    paramSample = torch.mul(torch.sqrt(torch.exp(mod.paramVar)), mod.stdNormalDistNS1.sample())\n",
    "\n",
    "    # Sample low-rank component (vectorized)\n",
    "    z2_samples = mod.stdNormalDistNS2.sample(sample_shape=torch.Size([mod.ns]))\n",
    "    lowrank_component = torch.matmul(mod.V, torch.mul(diagC_sqrt.unsqueeze(1), z2_samples.T))\n",
    "    paramSample = paramSample + lowrank_component.T\n",
    "    \n",
    "    ceVal = 0.0\n",
    "    \n",
    "    # Calculate log prior density of lambdaSq multiplied by Jacobian determinant\n",
    "    sampledLambda = torch.exp(0.5*(mod.scaleSqMu[0] + paramSample[:,0]))\n",
    "    ceVal = (ceVal + (-1.0/mod.ns)*torch.sum(torch.log(torch.mul(0.5*sampledLambda,\n",
    "            ((2.0/(math.pi)))/(1.0 + torch.square(sampledLambda))))))\n",
    "    \n",
    "    # Calculate log prior density of sigmaSq multiplied by Jacobian determinant\n",
    "    sampledSigma = torch.exp(mod.scaleSqMu[1] + paramSample[:,1])\n",
    "    ceVal = (ceVal + (-1.0/mod.ns)*torch.sum(torch.log(torch.mul(sampledSigma,\n",
    "            ((2.0/(math.pi)))/(1.0 + torch.square(sampledSigma))))))\n",
    "    \n",
    "    # Calculate log prior density of bias\n",
    "    sampledBeta = mod.betaMu[0] + paramSample[:,2]\n",
    "    ceVal = ceVal + (-1.0/mod.ns)*torch.sum(-0.5*torch.square(sampledBeta))\n",
    "    ceVal = ceVal + (-1.0/mod.ns)*(-1.0*mod.ns)*0.5*math.log(2.0*math.pi)\n",
    "    \n",
    "    # Calculate log prior density of regression coefficients\n",
    "    sampledLambda = torch.exp((1.0-mod.xi)*(mod.scaleSqMu[0] + paramSample[:,0]))\n",
    "    sampledBeta = mod.betaMu[1:].repeat(mod.ns,1) + paramSample[:,3:]\n",
    "    priorVar = torch.unsqueeze(sampledLambda*sampledSigma,1).repeat(1,mod.nw-1)\n",
    "    ceVal = (ceVal + (-1.0/mod.ns)*torch.sum(torch.mul(-0.5*torch.square(sampledBeta),\n",
    "             torch.pow(priorVar,-1.0))))\n",
    "    ceVal = (ceVal + (-1.0/mod.ns)*torch.sum(-0.5*(mod.nw-1)*(math.log(2.0*math.pi)\n",
    "          + torch.log(sampledLambda*sampledSigma))))\n",
    "    \n",
    "    return ceVal\n",
    "\n",
    "\n",
    "# Calculates the entropy\n",
    "def entropy(mod):\n",
    "    diagC = torch.diag(torch.exp(mod.C))\n",
    "    entVal = (0.5*(mod.nw+2) + 0.5*(mod.nw+2)*math.log(2.0*math.pi) + 0.5*torch.sum(mod.paramVar)\n",
    "           + 0.5*torch.logdet(torch.eye(mod.nc) + torch.matmul(torch.matmul(torch.transpose(mod.V,0,1),\n",
    "             torch.diag(torch.pow(torch.exp(mod.paramVar),-1.0))),torch.matmul(mod.V, diagC)))\n",
    "           + 0.5*torch.sum(mod.C))\n",
    "    return entVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08811529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the linear model using Stochastic Variational Inference\n",
    "# maxEpochs is the maximum number of training epochs to use\n",
    "def trainModel(mod,opt,maxEpochs,batchSize,intervalToPrint):\n",
    "    numBatches = math.floor(mod.Y.size(dim=0)/batchSize)\n",
    "    trainN = mod.Y.size(dim=0)\n",
    "    testN = mod.testY.size(dim=0)\n",
    "    tol = 0.0\n",
    "    nelboIdx = 0\n",
    "    nelboList = []\n",
    "    nllList = []\n",
    "    klList = []\n",
    "    nelboList.append(sys.float_info.max)\n",
    "    nllList.append(sys.float_info.max)\n",
    "    klList.append(sys.float_info.max)\n",
    "    for epoch in range(maxEpochs):\n",
    "        idx = torch.randperm(mod.Y.size(dim=0))\n",
    "        mod.Y = mod.Y[idx].view(mod.Y.shape)\n",
    "        mod.X = mod.X[idx].view(mod.X.shape)\n",
    "        for batch in range(numBatches):\n",
    "            opt.zero_grad()\n",
    "            NLLval = (1.0/batchSize)*NLL(mod,batch*batchSize,(batch + 1)*batchSize)\n",
    "            entVal = (1.0/trainN)*entropy(mod)\n",
    "            ceVal = (1.0/trainN)*crossEntropy(mod)\n",
    "            loss = NLLval + ceVal - entVal\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            with torch.no_grad():\n",
    "                mod.xi.clamp_(min = 0.0)\n",
    "                mod.xi.clamp_(max = 1.0)\n",
    "        if (epoch % intervalToPrint == 0):\n",
    "            with torch.no_grad():\n",
    "                NLLval = NLL(mod,0,trainN)\n",
    "                entVal = entropy(mod)\n",
    "                ceVal = crossEntropy(mod)\n",
    "                priorKL = ceVal - entVal\n",
    "                trainNELBO = NLLval + priorKL\n",
    "                nelboList.append(trainNELBO)\n",
    "                nllList.append(NLLval)\n",
    "                klList.append(priorKL)\n",
    "                nelboIdx = nelboIdx + 1\n",
    "                weights = sampleWeights(mod)\n",
    "                predTrainY = torch.matmul(mod.X,weights)\n",
    "                predTestY = torch.matmul(mod.testX,weights)\n",
    "                trainMSE = torch.mean(torch.square(mod.Y - predTrainY))\n",
    "                testMSE = torch.mean(torch.square(mod.testY - predTestY))\n",
    "                \n",
    "            print('Epoch {}, Neg Train ELBO {}, PriorKL {}, Train MSE {}, Test MSE {}'.format(epoch,trainNELBO,priorKL,trainMSE,testMSE))\n",
    "            # Calculate marginal variances using diagonal plus low rank covariance structure\n",
    "            marginalVar = torch.exp(mod.paramVar) + torch.sum(torch.square(mod.V)*torch.exp(mod.C).unsqueeze(0),dim=1)\n",
    "            print('Lambda Sq: {:.4e}'.format(torch.exp(mod.scaleSqMu[0] + 0.5*marginalVar[0])))\n",
    "            print('Sigma Sq: {:.4f}'.format(torch.exp(mod.scaleSqMu[1] + 0.5*marginalVar[1])))\n",
    "            print(mod.xi)\n",
    "            \n",
    "    return mod, nelboList, nllList, klList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f613d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2700, 1772])\n",
      "torch.Size([300, 1772])\n",
      "torch.Size([2700, 1])\n",
      "torch.Size([300, 1])\n",
      "Training size: 2700\n",
      "Testing size: 300\n",
      "tensor(1.1458, dtype=torch.float64)\n",
      "tensor(1.1787, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "trainFeatures = pd.read_csv(\"trainImagingFeatures.csv\", index_col = False)\n",
    "trainFeatures = trainFeatures.drop(trainFeatures.columns[0], axis = 1)\n",
    "trainFeatures = torch.tensor(trainFeatures.values, dtype = torch.double)\n",
    "trainFeatures = torch.cat((torch.ones((trainFeatures.size(dim=0),1)),trainFeatures),1)\n",
    "\n",
    "testFeatures = pd.read_csv(\"testImagingFeatures.csv\", index_col = False)\n",
    "testFeatures = testFeatures.drop(testFeatures.columns[0], axis = 1)\n",
    "testFeatures = torch.tensor(testFeatures.values, dtype = torch.double)\n",
    "testFeatures = torch.cat((torch.ones((testFeatures.size(dim=0),1)),testFeatures),1)\n",
    "\n",
    "trainResponse = pd.read_csv(\"train_y_residualized.csv\", index_col = False)\n",
    "trainResponse = trainResponse.drop(trainResponse.columns[0], axis = 1)\n",
    "trainResponse = torch.tensor(trainResponse.values, dtype = torch.double)\n",
    "\n",
    "testResponse = pd.read_csv(\"test_y_residualized.csv\", index_col = False)\n",
    "testResponse = testResponse.drop(testResponse.columns[0], axis = 1)\n",
    "testResponse = torch.tensor(testResponse.values, dtype = torch.double)\n",
    "\n",
    "print(trainFeatures.shape)\n",
    "print(testFeatures.shape)\n",
    "print(trainResponse.shape)\n",
    "print(testResponse.shape)\n",
    "\n",
    "print(f\"Training size: {trainFeatures.shape[0]}\")\n",
    "print(f\"Testing size: {testFeatures.shape[0]}\")\n",
    "\n",
    "print(torch.std(trainResponse))\n",
    "print(torch.std(testResponse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db3ab9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Neg Train ELBO 20263.21444554829, PriorKL 476.10421698104165, Train MSE 1.3916236666632558, Test MSE 1.5837941729874643\n",
      "Lambda Sq: 1.7843e-05\n",
      "Sigma Sq: 1.1492\n",
      "Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)\n",
      "Epoch 10, Neg Train ELBO 5930.0473100476065, PriorKL 408.12636387654084, Train MSE 1.172045627335924, Test MSE 1.2994745264662824\n",
      "Lambda Sq: 6.9089e-06\n",
      "Sigma Sq: 1.9770\n",
      "Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)\n",
      "Epoch 20, Neg Train ELBO 5287.085937406418, PriorKL 350.4203944945298, Train MSE 1.1346180589830857, Test MSE 1.2521084987788593\n",
      "Lambda Sq: 5.0551e-06\n",
      "Sigma Sq: 2.0663\n",
      "Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)\n",
      "Epoch 30, Neg Train ELBO 4968.156020422975, PriorKL 277.3412075725246, Train MSE 1.1236243631699623, Test MSE 1.236177401383481\n",
      "Lambda Sq: 3.9013e-06\n",
      "Sigma Sq: 1.9667\n",
      "Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)\n",
      "Epoch 40, Neg Train ELBO 4779.642677960959, PriorKL 213.5558234947389, Train MSE 1.1140718372860454, Test MSE 1.2267253330630206\n",
      "Lambda Sq: 3.1020e-06\n",
      "Sigma Sq: 1.8013\n",
      "Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "intervalToPrint = 10\n",
    "batchSize = 256\n",
    "numComponents = 16\n",
    "numCESamples = 128\n",
    "numNLLSamples = 2\n",
    "xiVal = 1.0\n",
    "\n",
    "# initialize model\n",
    "model = Model(trainFeatures,testFeatures,trainResponse,testResponse,numComponents,numCESamples,numNLLSamples,xiVal)\n",
    "\n",
    "# first train at low batch size\n",
    "maxEpochs = 50\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.025)\n",
    "model, nelboList, nllList, klList = trainModel(model,optimizer,maxEpochs,batchSize,intervalToPrint)\n",
    "\n",
    "# Save\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'nelboList': nelboList,\n",
    "    'nllList': nllList,\n",
    "    'klList': klList\n",
    "}, 'model_and_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e4089d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Neg Train ELBO 4598.262883966667, PriorKL 147.98498616995676, Train MSE 1.1135179401693907, Test MSE 1.2242833827276076\n",
      "Lambda Sq: 2.5063e-06\n",
      "Sigma Sq: 1.6425\n",
      "Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)\n",
      "Epoch 10, Neg Train ELBO 4522.657215756153, PriorKL 155.8791110097377, Train MSE 1.107753984179401, Test MSE 1.223186116189867\n",
      "Lambda Sq: 2.0688e-06\n",
      "Sigma Sq: 1.5194\n",
      "Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)\n",
      "Epoch 20, Neg Train ELBO 4452.884849105958, PriorKL 143.91623047153962, Train MSE 1.1074120217205328, Test MSE 1.220378178847819\n",
      "Lambda Sq: 1.7258e-06\n",
      "Sigma Sq: 1.4305\n",
      "Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "intervalToPrint = 10\n",
    "batchSize = 256\n",
    "numComponents = 16\n",
    "numCESamples = 128\n",
    "numNLLSamples = 2\n",
    "xiVal = 1.0\n",
    "\n",
    "# Load and continue\n",
    "checkpoint = torch.load('model_and_metrics.pt')\n",
    "model = Model(trainFeatures, testFeatures, trainResponse, testResponse, \n",
    "              numComponents, numCESamples, numNLLSamples, xiVal)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "nelboList = checkpoint['nelboList']\n",
    "nllList = checkpoint['nllList']\n",
    "klList = checkpoint['klList']\n",
    "\n",
    "# continue training model\n",
    "maxEpochs = 30\n",
    "model, nelboList, nllList, klList = trainModel(model,optimizer,maxEpochs,batchSize,intervalToPrint)\n",
    "\n",
    "# Save\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'nelboList': nelboList,\n",
    "    'nllList': nllList,\n",
    "    'klList': klList\n",
    "}, 'model_and_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da258c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
