{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2950609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from torch.nn.utils.parametrizations import _Orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539cf84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined model class that inherits from Pytorch's neural network module\n",
    "# NumComponents is the number of low rank components to use in the low rank\n",
    "# part of the diagonal plus low rank covariance matrix.\n",
    "# numCESamples is the number of stochastic samples of the cross entropy\n",
    "# numNLLSamples is the number of stochastic samples of the negative log likelihood.\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,trainX,testX,trainY,testY,numComponents,numCESamples,numNLLSamples,xiVal):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input data\n",
    "        self.X = trainX\n",
    "        self.testX = testX\n",
    "        self.Y = trainY\n",
    "        self.testY = testY\n",
    "        self.trainN = self.Y.size(dim=0)\n",
    "        self.testN = self.testY.size(dim=0)\n",
    "        self.nw = self.X.size(dim=1)\n",
    "        self.nc = numComponents\n",
    "        self.ns = numCESamples\n",
    "        self.nsNLL = numNLLSamples\n",
    "        \n",
    "        # Regression parameter means\n",
    "        betaInit = torch.squeeze(torch.distributions.Uniform(-0.0001,0.0001).sample((1,self.nw)).double())\n",
    "        self.betaMu = nn.Parameter(betaInit)\n",
    "        self.scaleSqMu = nn.Parameter(torch.tensor([math.log(0.000025)-0.5,-0.5],dtype=torch.double))\n",
    "        self.xi = nn.Parameter(torch.tensor(xiVal,dtype=torch.double))\n",
    "        \n",
    "        # Diagonal component of parameter covariance\n",
    "        self.paramVar = nn.Parameter(torch.ones((self.nw+2),dtype=torch.double)*0.0001)\n",
    "        \n",
    "        # Low rank covariance of all parameters\n",
    "        self.V = nn.Parameter(torch.distributions.Uniform(-0.0001,0.0001).sample((self.nw+2,self.nc)).double())\n",
    "        \n",
    "        # Diagonal matrix of eigenvalues for low rank component\n",
    "        self.C = nn.Parameter(torch.ones(self.nc,dtype=torch.double)*-10.0)\n",
    "        \n",
    "        # Apply orthogonal parametrization to V\n",
    "        # Use householder reflections instead of the matrix exponential or Cayley map as householder\n",
    "        # reflections are computationally cheaper\n",
    "        parametrize.register_parametrization(self,\"V\",_Orthogonal(self.V,orthogonal_map=\"householder\"))\n",
    "        \n",
    "        # Scale parameter priors\n",
    "        self.lambdaPrior = torch.tensor(1.0,dtype=torch.double)\n",
    "        self.regVarPrior = torch.tensor(1.0,dtype=torch.double)\n",
    "        \n",
    "        # Training metrics\n",
    "        self.listTrainMSE = []\n",
    "        self.listTestMSE = []\n",
    "        self.listTrainR2 = []\n",
    "        self.listTestR2 = []\n",
    "        \n",
    "\n",
    "# Samples the weights wrt to the q density\n",
    "def sampleWeights(mod):\n",
    "    diagC = torch.diag(torch.exp(mod.C))\n",
    "    mod.paramCov = torch.diag(torch.exp(mod.paramVar)) + torch.matmul(torch.matmul(mod.V,diagC),torch.transpose(mod.V,0,1))\n",
    "    lsCov = mod.paramCov[0:2,0:2]\n",
    "     \n",
    "    predWeights = 0.0\n",
    "    for i in range(mod.ns):\n",
    "        \n",
    "        # Sample diagonal lambdaSq and sigmaSq\n",
    "        mod.stdNormalDistNP = torch.distributions.MultivariateNormal(mod.scaleSqMu,covariance_matrix = lsCov)\n",
    "        lsParams = mod.stdNormalDistNP.rsample()\n",
    "        \n",
    "        # Sample weights\n",
    "        predBeta = (torch.unsqueeze(mod.betaMu,1) + torch.matmul(mod.paramCov[2:,0:2],\n",
    "                    torch.matmul(torch.inverse(lsCov),torch.unsqueeze(lsParams,1) - torch.unsqueeze(mod.scaleSqMu,1))))\n",
    "        scaleParams = torch.unsqueeze(torch.cat(((torch.unsqueeze(torch.tensor(1.0,dtype=torch.double),0)).clone(),\n",
    "                      torch.exp(0.5*mod.xi*lsParams[0])*torch.ones((mod.nw-1))),0),1)\n",
    "        predWeights = predWeights + torch.mul(predBeta,scaleParams)\n",
    "        \n",
    "    return (1.0/mod.ns)*predWeights\n",
    "\n",
    "\n",
    "# Calculate posterior means of regression coefficients accounting for partial centering\n",
    "def compute_posterior_means(mod,num_samples):\n",
    "    \n",
    "    # Calculate parameter covariance\n",
    "    diagC = torch.diag(torch.exp(model.C))\n",
    "    paramCov = (torch.diag(torch.exp(model.paramVar))\n",
    "                + torch.matmul(torch.matmul(model.V,diagC),torch.transpose(model.V,0,1)))\n",
    "    \n",
    "    # Covariance for scale parameters\n",
    "    lsCov = paramCov[0:2,0:2]\n",
    "    \n",
    "    # Initialize storage for all samples\n",
    "    all_samples = torch.zeros((num_samples,model.nw),dtype=torch.double)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Sample scale parameters\n",
    "            dist = torch.distributions.MultivariateNormal(model.scaleSqMu,covariance_matrix=lsCov)\n",
    "            lsParams = dist.rsample()\n",
    "            \n",
    "            # Calculate conditional mean\n",
    "            predBeta = (torch.unsqueeze(model.betaMu,1) + torch.matmul(paramCov[2:,0:2],\n",
    "                        torch.matmul(torch.inverse(lsCov),torch.unsqueeze(lsParams,1) - torch.unsqueeze(model.scaleSqMu,1))))\n",
    "            \n",
    "            # Partially center the regression coefficients but not the intercept\n",
    "            scaleParams = torch.unsqueeze(torch.cat((torch.unsqueeze(torch.tensor(1.0,dtype=torch.double),0),\n",
    "                                                     torch.exp(0.5*model.xi*lsParams[0])*torch.ones((model.nw - 1))),0),1)\n",
    "            \n",
    "            # Calculate true regression coefficients accounting for partial centering\n",
    "            weights = torch.mul(predBeta,scaleParams)\n",
    "            all_samples[i,:] = weights.squeeze()\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    posterior_mean = all_samples.mean(dim=0,keepdim=True).T\n",
    "    ci_lower = torch.quantile(all_samples,0.025,dim=0,keepdim=True).T\n",
    "    ci_upper = torch.quantile(all_samples,0.975,dim=0,keepdim=True).T\n",
    "    \n",
    "    return posterior_mean,ci_lower,ci_upper\n",
    "\n",
    "\n",
    "# Calculates the negative log likelihood\n",
    "def NLL(mod,startBatch,endBatch):\n",
    "    diagC = torch.diag(torch.exp(mod.C))\n",
    "    diagC_sqrt = torch.diag(torch.sqrt(torch.exp(mod.C)))\n",
    "    mod.paramCov = torch.diag(torch.exp(mod.paramVar)) + torch.matmul(torch.matmul(mod.V,diagC),torch.transpose(mod.V,0,1))\n",
    "    lsCov = mod.paramCov[0:2,0:2]\n",
    "    \n",
    "    # Calculate K matrix to be re-used in the negative log likelihood calculation\n",
    "    K = (torch.diag(torch.pow(torch.exp(mod.C),-1.0))\n",
    "         + torch.matmul(torch.matmul(torch.transpose(mod.V[0:2,:],0,1),\n",
    "                                     torch.diag(torch.pow(torch.exp(mod.paramVar[0:2]),-1.0))),mod.V[0:2,:]))\n",
    "    LK = torch.linalg.cholesky(K)\n",
    "    mod.stdNormalDistNP = torch.distributions.MultivariateNormal(mod.scaleSqMu,covariance_matrix = lsCov)\n",
    "    \n",
    "    # Calculate negative log likelihood for repeated samples from the q density\n",
    "    nll = 0.0\n",
    "    for i in range(mod.nsNLL):\n",
    "        \n",
    "        # Sample diagonal lambdaSq and sigmaSq\n",
    "        lsParams = mod.stdNormalDistNP.rsample(sample_shape=torch.Size([endBatch-startBatch]))\n",
    "        \n",
    "        ### Calculate pre-activation distribution\n",
    "        # Calculate mean beta conditional on sampled scale parameters\n",
    "        predBeta = (torch.unsqueeze(mod.betaMu,1) + torch.matmul(mod.paramCov[2:,0:2],\n",
    "                    torch.matmul(torch.inverse(lsCov),torch.transpose(lsParams,0,1)\n",
    "                 - torch.unsqueeze(mod.scaleSqMu,1).repeat(1,endBatch-startBatch))))\n",
    "        scaleData = torch.cat(((torch.unsqueeze(mod.X[startBatch:endBatch,0],1)).clone(),\n",
    "                    torch.unsqueeze(torch.exp(0.5*mod.xi*lsParams[:,0]),1)*mod.X[startBatch:endBatch,1:]),1)\n",
    "        predMu = torch.sum(torch.mul(torch.transpose(predBeta,0,1),scaleData),dim=1)\n",
    "        \n",
    "        Y = torch.linalg.solve(torch.transpose(LK,0,1),torch.matmul(torch.matmul(torch.matmul(torch.transpose(mod.V[0:2,:],0,1),\n",
    "            torch.diag(torch.pow(torch.exp(mod.paramVar[0:2]),-1))),torch.matmul(mod.V[0:2,:],diagC_sqrt)),\n",
    "            torch.matmul(torch.matmul(diagC_sqrt,torch.transpose(mod.V[2:,:],0,1)),torch.transpose(scaleData,0,1))))\n",
    "        \n",
    "        # Standard deviation calculation with updated low-rank component\n",
    "        # Use sqrt(C) instead of C when calculating standard deviation of pre-activations\n",
    "        term1 = torch.sum(torch.square(torch.matmul(torch.diag(torch.exp(0.5*mod.paramVar[2:])),\n",
    "                                                    torch.transpose(scaleData,0,1))),dim=0)\n",
    "        term2 = torch.sum(torch.square(torch.matmul(torch.matmul(diagC_sqrt,torch.transpose(mod.V[2:,:],0,1)),\n",
    "                                                    torch.transpose(scaleData,0,1))),dim=0)\n",
    "        temp3 = torch.matmul(torch.matmul(diagC_sqrt,torch.transpose(mod.V[2:,:],0,1)),\n",
    "                             torch.transpose(scaleData,0,1))\n",
    "        temp3 = torch.matmul(torch.matmul(mod.V[0:2,:],diagC_sqrt),temp3)\n",
    "        temp3 = torch.matmul(torch.diag(torch.pow(torch.exp(mod.paramVar[0:2]),-0.5)),temp3)\n",
    "        term3 = torch.sum(torch.square(temp3),dim=0)\n",
    "        term4 = torch.sum(torch.square(Y),dim=0)\n",
    "        sdB = torch.sqrt(term1 + term2 - term3 + term4)\n",
    "        \n",
    "        # Sample pre-activation and calculate negative log-likelihood\n",
    "        mod.stdNormalDistB = torch.distributions.Normal(predMu,sdB)\n",
    "        sampleB = mod.stdNormalDistB.rsample()\n",
    "        trainResiduals = mod.Y[startBatch:endBatch,:] - torch.unsqueeze(sampleB,1)\n",
    "        SE = torch.square(trainResiduals)\n",
    "        # Calculate log likelihood\n",
    "        nll = (nll + torch.sum(torch.mul(0.5*torch.unsqueeze(torch.exp(-1.0*lsParams[:,1]),1),SE)\n",
    "            + 0.5*(math.log(2.0*math.pi) + torch.unsqueeze(lsParams[:,1],1))))\n",
    "        \n",
    "    return (1.0/mod.nsNLL)*nll\n",
    "\n",
    "\n",
    "# Calculates the cross entropy\n",
    "def crossEntropy(mod):\n",
    "        \n",
    "    ## Sample all parameters together\n",
    "    mod.stdNormalDistNS1 = torch.distributions.Normal(torch.zeros((mod.ns,mod.nw+2),dtype=torch.double),\n",
    "                                                      torch.ones((mod.ns,mod.nw+2),dtype=torch.double))\n",
    "    mod.stdNormalDistNS2 = torch.distributions.Normal(torch.zeros((mod.nc),dtype=torch.double),\n",
    "                                                      torch.ones((mod.nc),dtype=torch.double))\n",
    "    \n",
    "    # Sample diagonal component\n",
    "    paramSample = torch.mul(torch.sqrt(torch.exp(mod.paramVar)),mod.stdNormalDistNS1.sample())\n",
    "\n",
    "    # Sample low-rank component\n",
    "    diagC_sqrt = torch.sqrt(torch.exp(mod.C))\n",
    "    z2_samples = mod.stdNormalDistNS2.sample(sample_shape=torch.Size([mod.ns]))\n",
    "    lowrank_component = torch.matmul(mod.V,torch.mul(diagC_sqrt.unsqueeze(1),z2_samples.T))\n",
    "    paramSample = paramSample + lowrank_component.T\n",
    "    \n",
    "    ceVal = 0.0\n",
    "    \n",
    "    # Calculate log prior density of lambdaSq multiplied by Jacobian determinant\n",
    "    sampledLambda = torch.exp(0.5*(mod.scaleSqMu[0] + paramSample[:,0]))\n",
    "    ceVal = (ceVal + (-1.0/mod.ns)*torch.sum(torch.log(torch.mul(0.5*sampledLambda,\n",
    "            ((2.0/(math.pi)))/(1.0 + torch.square(sampledLambda))))))\n",
    "    \n",
    "    # Calculate log prior density of sigmaSq multiplied by Jacobian determinant\n",
    "    sampledSigma = torch.exp(mod.scaleSqMu[1] + paramSample[:,1])\n",
    "    ceVal = (ceVal + (-1.0/mod.ns)*torch.sum(torch.log(torch.mul(sampledSigma,\n",
    "            ((2.0/(math.pi)))/(1.0 + torch.square(sampledSigma))))))\n",
    "    \n",
    "    # Calculate log prior density of bias\n",
    "    sampledBeta = mod.betaMu[0] + paramSample[:,2]\n",
    "    ceVal = ceVal + (-1.0/mod.ns)*torch.sum(-0.5*torch.square(sampledBeta))\n",
    "    ceVal = ceVal + (-1.0/mod.ns)*(-1.0*mod.ns)*0.5*math.log(2.0*math.pi)\n",
    "    \n",
    "    # Calculate log prior density of regression coefficients\n",
    "    sampledLambda = torch.exp((1.0-mod.xi)*(mod.scaleSqMu[0] + paramSample[:,0]))\n",
    "    sampledBeta = mod.betaMu[1:].repeat(mod.ns,1) + paramSample[:,3:]\n",
    "    priorVar = torch.unsqueeze(sampledLambda*sampledSigma,1).repeat(1,mod.nw-1)\n",
    "    ceVal = (ceVal + (-1.0/mod.ns)*torch.sum(torch.mul(-0.5*torch.square(sampledBeta),\n",
    "             torch.pow(priorVar,-1.0))))\n",
    "    ceVal = (ceVal + (-1.0/mod.ns)*torch.sum(-0.5*(mod.nw-1)*(math.log(2.0*math.pi)\n",
    "          + torch.log(sampledLambda*sampledSigma))))\n",
    "    \n",
    "    return ceVal\n",
    "\n",
    "\n",
    "# Calculates the entropy\n",
    "def entropy(mod):\n",
    "    diagC = torch.diag(torch.exp(mod.C))\n",
    "    entVal = (0.5*(mod.nw+2) + 0.5*(mod.nw+2)*math.log(2.0*math.pi) + 0.5*torch.sum(mod.paramVar)\n",
    "           + 0.5*torch.logdet(torch.eye(mod.nc) + torch.matmul(torch.matmul(torch.transpose(mod.V,0,1),\n",
    "             torch.diag(torch.pow(torch.exp(mod.paramVar),-1.0))),torch.matmul(mod.V,diagC)))\n",
    "           + 0.5*torch.sum(mod.C))\n",
    "    return entVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08811529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the linear model using Stochastic Variational Inference\n",
    "# maxEpochs is the maximum number of training epochs to use\n",
    "def trainModel(mod,opt,maxEpochs,batchSize,intervalToPrint):\n",
    "    numBatches = math.floor(mod.Y.size(dim=0)/batchSize)\n",
    "    trainN = mod.Y.size(dim=0)\n",
    "    testN = mod.testY.size(dim=0)\n",
    "    tol = 0.0\n",
    "    nelboIdx = 0\n",
    "    nelboList = []\n",
    "    nllList = []\n",
    "    klList = []\n",
    "    nelboList.append(sys.float_info.max)\n",
    "    nllList.append(sys.float_info.max)\n",
    "    klList.append(sys.float_info.max)\n",
    "    for epoch in range(maxEpochs):\n",
    "        idx = torch.randperm(mod.Y.size(dim=0))\n",
    "        mod.Y = mod.Y[idx].view(mod.Y.shape)\n",
    "        mod.X = mod.X[idx].view(mod.X.shape)\n",
    "        for batch in range(numBatches):\n",
    "            opt.zero_grad()\n",
    "            NLLval = (1.0/batchSize)*NLL(mod,batch*batchSize,(batch + 1)*batchSize)\n",
    "            entVal = (1.0/trainN)*entropy(mod)\n",
    "            ceVal = (1.0/trainN)*crossEntropy(mod)\n",
    "            loss = NLLval + ceVal - entVal\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            with torch.no_grad():\n",
    "                # Prior parameterization must be between 0 and 1 inclusive\n",
    "                mod.xi.clamp_(min = 0.0)\n",
    "                mod.xi.clamp_(max = 1.0)\n",
    "        if (epoch % intervalToPrint == 0):\n",
    "            with torch.no_grad():\n",
    "                NLLval = NLL(mod,0,trainN)\n",
    "                entVal = entropy(mod)\n",
    "                ceVal = crossEntropy(mod)\n",
    "                priorKL = ceVal - entVal\n",
    "                trainNELBO = NLLval + priorKL\n",
    "                nelboList.append(trainNELBO)\n",
    "                nllList.append(NLLval)\n",
    "                klList.append(priorKL)\n",
    "                nelboIdx = nelboIdx + 1\n",
    "                weights = sampleWeights(mod)\n",
    "                predTrainY = torch.matmul(mod.X,weights)\n",
    "                predTestY = torch.matmul(mod.testX,weights)\n",
    "                trainMSE = torch.mean(torch.square(mod.Y - predTrainY))\n",
    "                testMSE = torch.mean(torch.square(mod.testY - predTestY))\n",
    "                \n",
    "            print('Epoch {}, Neg Train ELBO {}, PriorKL {}, Train MSE {}, Test MSE {}'.format(epoch,trainNELBO,priorKL,trainMSE,testMSE))\n",
    "            # Calculate marginal variances using diagonal plus low rank covariance structure\n",
    "            marginalVar = torch.exp(mod.paramVar) + torch.sum(torch.square(mod.V)*torch.exp(mod.C).unsqueeze(0),dim=1)\n",
    "            print('Lambda Sq: {:.4e}'.format(torch.exp(mod.scaleSqMu[0] + 0.5*marginalVar[0])))\n",
    "            print('Sigma Sq: {:.4f}'.format(torch.exp(mod.scaleSqMu[1] + 0.5*marginalVar[1])))\n",
    "            print(mod.xi)\n",
    "            \n",
    "    return mod,nelboList,nllList,klList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f613d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2700, 1772])\n",
      "torch.Size([300, 1772])\n",
      "torch.Size([2700, 1])\n",
      "torch.Size([300, 1])\n",
      "Training size: 2700\n",
      "Testing size: 300\n",
      "tensor(1.1458, dtype=torch.float64)\n",
      "tensor(1.1787, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "trainFeatures = pd.read_csv(\"trainImagingFeatures.csv\", index_col = False)\n",
    "trainFeatures = trainFeatures.drop(trainFeatures.columns[0], axis = 1)\n",
    "trainFeatures = torch.tensor(trainFeatures.values, dtype = torch.double)\n",
    "trainFeatures = torch.cat((torch.ones((trainFeatures.size(dim=0),1)),trainFeatures),1)\n",
    "\n",
    "testFeatures = pd.read_csv(\"testImagingFeatures.csv\", index_col = False)\n",
    "testFeatures = testFeatures.drop(testFeatures.columns[0], axis = 1)\n",
    "testFeatures = torch.tensor(testFeatures.values, dtype = torch.double)\n",
    "testFeatures = torch.cat((torch.ones((testFeatures.size(dim=0),1)),testFeatures),1)\n",
    "\n",
    "trainResponse = pd.read_csv(\"train_y_residualized.csv\", index_col = False)\n",
    "trainResponse = trainResponse.drop(trainResponse.columns[0], axis = 1)\n",
    "trainResponse = torch.tensor(trainResponse.values, dtype = torch.double)\n",
    "\n",
    "testResponse = pd.read_csv(\"test_y_residualized.csv\", index_col = False)\n",
    "testResponse = testResponse.drop(testResponse.columns[0], axis = 1)\n",
    "testResponse = torch.tensor(testResponse.values, dtype = torch.double)\n",
    "\n",
    "print(trainFeatures.shape)\n",
    "print(testFeatures.shape)\n",
    "print(trainResponse.shape)\n",
    "print(testResponse.shape)\n",
    "\n",
    "print(f\"Training size: {trainFeatures.shape[0]}\")\n",
    "print(f\"Testing size: {testFeatures.shape[0]}\")\n",
    "\n",
    "print(torch.std(trainResponse))\n",
    "print(torch.std(testResponse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ab9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intervalToPrint = 10\n",
    "batchSize = 256\n",
    "numComponents = 16\n",
    "numCESamples = 128\n",
    "numNLLSamples = 2\n",
    "xiVal = 1.0\n",
    "\n",
    "# initialize model\n",
    "model = Model(trainFeatures,testFeatures,trainResponse,testResponse,numComponents,numCESamples,numNLLSamples,xiVal)\n",
    "\n",
    "# first train at low batch size\n",
    "maxEpochs = 50\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.025)\n",
    "model,nelboList,nllList,klList = trainModel(model,optimizer,maxEpochs,batchSize,intervalToPrint)\n",
    "\n",
    "# Save\n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'nelboList': nelboList,\n",
    "            'nllList': nllList,\n",
    "            'klList': klList},'model_and_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4089d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intervalToPrint = 10\n",
    "batchSize = 512\n",
    "numComponents = 16\n",
    "numCESamples = 128\n",
    "numNLLSamples = 2\n",
    "xiVal = 1.0\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load('model_and_metrics.pt')\n",
    "model = Model(trainFeatures,testFeatures,trainResponse,testResponse,\n",
    "              numComponents,numCESamples,numNLLSamples,xiVal)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.005)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "nelboList = checkpoint['nelboList']\n",
    "nllList = checkpoint['nllList']\n",
    "klList = checkpoint['klList']\n",
    "\n",
    "# Continue training model\n",
    "maxEpochs = 60\n",
    "model,nelboList,nllList,klList = trainModel(model,optimizer,maxEpochs,batchSize,intervalToPrint)\n",
    "\n",
    "# Save\n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'nelboList': nelboList,\n",
    "            'nllList': nllList,\n",
    "            'klList': klList},'model_and_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a17a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervalToPrint = 10\n",
    "batchSize = 512\n",
    "numComponents = 16\n",
    "numCESamples = 128\n",
    "numNLLSamples = 2\n",
    "xiVal = 1.0\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load('model_and_metrics.pt')\n",
    "model = Model(trainFeatures,testFeatures,trainResponse,testResponse,\n",
    "              numComponents,numCESamples,numNLLSamples,xiVal)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.005)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Sample the true regression parameters (accounting for partial centering) using the q density\n",
    "posterior_mean,ci_lower,ci_upper = compute_posterior_means(model,num_samples=4000)\n",
    "\n",
    "# Convert to numpy\n",
    "mean_np = posterior_mean.squeeze().cpu().numpy()\n",
    "lower_np = ci_lower.squeeze().cpu().numpy()\n",
    "upper_np = ci_upper.squeeze().cpu().numpy()\n",
    "\n",
    "# Save posterior means\n",
    "np.savetxt('posterior_mean_svi.csv',mean_np,delimiter=',')\n",
    "np.savetxt('posterior_ci_lower_svi.csv',lower_np,delimiter=',')\n",
    "np.savetxt('posterior_ci_upper_svi.csv',upper_np,delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
